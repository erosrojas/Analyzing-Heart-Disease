{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0800b17-27cf-4aac-8f98-4bac1c98cf7b",
   "metadata": {},
   "source": [
    "# Project Proposal: Predicting Heart Disease"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe157a10-3b27-4ab5-b81c-337b68ba1708",
   "metadata": {},
   "source": [
    "**Group 11**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08870d39-e319-490d-a1ef-a017e6c3ee2a",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248b2564-d7e6-4897-a83a-6d549bbfad06",
   "metadata": {},
   "source": [
    "Heart or cardiovascular disease is known as one of the leading causes of death for people of most racial and ethnic groups in the world<sup>4,11</sup>. This disease refers to any condition that affects the structure or function of the heart. The most common type is coronary heart disease, which occurs when there is narrowing of arteries and leads to heart attacks and chest pains or anginas<sup>5,11</sup>. Thus, how to prevent heart disease and how to accurately predict the probability of heart disease is an urgent and crucial scientific topic. In this project, we will answer the question: **What are the statistical correlations between given covariables, and how do their correlations impact the predictive power of logistic regression, specifically on whether a patient has heart disease or not?**\n",
    "\n",
    "With the prevalence of machine learning, it is gradually being applied to many industries including disease prediction and diagnosis.<sup>1,7</sup>. Current literature has demonstrated two contradicting results in heart disease prediction models. Some research shows that random forest and K-nearest neighbors produce the greatest accuracy<sup>2,6,7,10</sup> in contrast to the findings that multinomial logistic regression model outperformed other hybrid models<sup>9</sup>. Generally, it seems that logistic regression is not very popular, perhaps for its seemingly lower accuracy levels in comparison to other models, but there is no explanation as to why that is. Despite the lack of support for the strength in accuracy for logistic regression, past research does not necessarily suggest that it is an extremely bad choice. Thus, our main purpose is to seek to improve the accuracy of prediction of heart disease using logistic regression and minimize the amount of diagnostic errors to provide a better quality health case service to patients<sup>1</sup>.\n",
    "\n",
    "The data set we will be using to answer our question is found in the UCI Machine Learning Repository Heart Disease Data Set<sup>8</sup>. This dataset contains a diagnosis of heart disease for each patient, represented by variable num, some descriptive variables (eg. age), and numerous bodily measurements related to the heart, either categorical (eg. cp) or numerical (eg. chol).  The response variable (i.e. \"num\") and the 13 covariates are in Appendix I. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ffda91-b567-4fa0-ad56-14ca0af48eb6",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10813d1-a44a-466d-8559-7b354f684a7f",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── \u001b[1mAttaching packages\u001b[22m ─────────────────────────────────────── tidyverse 1.3.2 ──\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.3.6      \u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.5 \n",
      "\u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 3.1.8      \u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 1.0.10\n",
      "\u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 1.2.1      \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.4.1 \n",
      "\u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 2.1.3      \u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 0.5.2 \n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "Registered S3 method overwritten by 'GGally':\n",
      "  method from   \n",
      "  +.gg   ggplot2\n",
      "\n",
      "\n",
      "Attaching package: ‘gridExtra’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:dplyr’:\n",
      "\n",
      "    combine\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#install.packages(\"ggpubr\")\n",
    "library(tidyverse)\n",
    "library(GGally)\n",
    "library(gridExtra)\n",
    "library(cowplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1a73b6-e23c-429f-a0be-a44aa0066439",
   "metadata": {},
   "source": [
    "Loading in the data (replacing all \"?\" with NA since the predetermined NA value was \"?\", and NA is easier to deal with). Also, adding a new column `country` which will help keep track of which hospital the data came from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a0f9516-c92f-4767-942a-e9ced49f8063",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in open.connection(structure(4L, class = c(\"curl\", \"connection\"), conn_id = <pointer: 0x279>), : Timeout was reached: [archive.ics.uci.edu] Connection timeout after 10001 ms\n",
     "output_type": "error",
     "traceback": [
      "Error in open.connection(structure(4L, class = c(\"curl\", \"connection\"), conn_id = <pointer: 0x279>), : Timeout was reached: [archive.ics.uci.edu] Connection timeout after 10001 ms\nTraceback:\n",
      "1. read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data\", \n .     col_names = names, na = \"?\", show_col_types = FALSE) %>% \n .     mutate(country = 2)",
      "2. mutate(., country = 2)",
      "3. read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data\", \n .     col_names = names, na = \"?\", show_col_types = FALSE)",
      "4. vroom::vroom(file, delim = \",\", col_names = col_names, col_types = col_types, \n .     col_select = {\n .         {\n .             col_select\n .         }\n .     }, id = id, .name_repair = name_repair, skip = skip, n_max = n_max, \n .     na = na, quote = quote, comment = comment, skip_empty_rows = skip_empty_rows, \n .     trim_ws = trim_ws, escape_double = TRUE, escape_backslash = FALSE, \n .     locale = locale, guess_max = guess_max, show_col_types = show_col_types, \n .     progress = progress, altrep = lazy, num_threads = num_threads)",
      "5. vroom_(file, delim = delim %||% col_types$delim, col_names = col_names, \n .     col_types = col_types, id = id, skip = skip, col_select = col_select, \n .     name_repair = .name_repair, na = na, quote = quote, trim_ws = trim_ws, \n .     escape_double = escape_double, escape_backslash = escape_backslash, \n .     comment = comment, skip_empty_rows = skip_empty_rows, locale = locale, \n .     guess_max = guess_max, n_max = n_max, altrep = vroom_altrep(altrep), \n .     num_threads = num_threads, progress = progress)",
      "6. (function (con, ...) \n . UseMethod(\"open\"))(structure(4L, class = c(\"curl\", \"connection\"\n . ), conn_id = <pointer: 0x279>), \"rb\")",
      "7. open.connection(structure(4L, class = c(\"curl\", \"connection\"), conn_id = <pointer: 0x279>), \n .     \"rb\")"
     ]
    }
   ],
   "source": [
    "# Setting names of columns\n",
    "names <- c(\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"num\", show_col_types = FALSE)\n",
    "\n",
    "# Reading all of the data from https://archive.ics.uci.edu/ml/datasets/heart+Disease\n",
    "cleveland_data <- read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\", col_names = names, na = \"?\", show_col_types = FALSE) %>% mutate(country = 0)\n",
    "switzerland_data <- read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.switzerland.data\", col_names = names, na = \"?\", show_col_types = FALSE) %>% mutate(country = 1)\n",
    "hungary_data <- read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.hungarian.data\", col_names = names, na = \"?\", show_col_types = FALSE) %>% mutate(country = 2)\n",
    "va_data <- read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.va.data\", col_names = names, na = \"?\", show_col_types = FALSE) %>% mutate(country = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8b25f3-2c14-4cea-a450-1100a21812a7",
   "metadata": {},
   "source": [
    "Counts of NA values in each dataset, and within every column. This allows us to see which columns we should remove early on, and which would likely cause issues in terms of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8086b97-a83f-4579-88b2-9f07539346c4",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "NA_counts <- as.data.frame(cleveland_data[FALSE, ])\n",
    "x <- substitute(list(cleveland_data, switzerland_data, hungary_data, va_data))\n",
    "\n",
    "for (i in as.list(x)[-1]) {\n",
    "    NA_counts[nrow(NA_counts) + 1,] <- map_df(get(i), ~sum(is.na(.x)))\n",
    "    rownames(NA_counts)[nrow(NA_counts)] <- deparse(i)\n",
    "}\n",
    "NA_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2e0ac5-ea33-4ae8-96ec-089432562707",
   "metadata": {},
   "source": [
    "It looks like the columns `slope`, `ca`, and `thal` all have a large quantity of NA values within three of the four datasets that we are using. For this reason, we chose to drop these three columns because performing imputation with such a small quantity of usable data will lead to inaccurate results, and choosing to omit NA values will delete the rows entirely, making us lose a large number of rows. Additionally, we are choosing to drop `fbs`, `exang`, and `oldpeak` for a similar reason. As such, removing these columns will allow us to keep more observations for our analysis, as well as simplify our model later on by exchanging less features for more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcba14f2-cecf-48f8-9a6f-eded6a76a9a2",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "data_tidy_multi <- rbind(cleveland_data, switzerland_data, hungary_data, va_data) %>%\n",
    "    select(-slope, -ca, -thal, -fbs, -exang, -oldpeak) %>%\n",
    "    na.omit()\n",
    "\n",
    "num_counts_multi <- data_tidy_multi %>%\n",
    "    group_by(num) %>%\n",
    "    summarize(counts = n(), proportions = counts/nrow(data_tidy_multi))\n",
    "num_counts_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0294ea60-ede0-4118-ac18-b94b6851f6b5",
   "metadata": {},
   "source": [
    "We can see that there is quite a large class imbalance within the dataset, particularly in the cases of more severe heart disease. Since this analysis is mainly focused on finding whether patients have heart disease or not, we are not interested in the severity of their condition, rather whether or not heart disease is present at all. Therefore, we can binarize the output variable by grouping categories 1-4 into a single case (has heart disease)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803a390c-7cf0-450e-84dd-90910e1d5ee1",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "data_tidy <- data_tidy_multi %>%\n",
    "    mutate(num = ifelse(num == 0, 0, 1)) %>%\n",
    "    mutate(num = as_factor(num), cp = as_factor(cp), sex = as_factor(sex), restecg = as_factor(restecg), country = as_factor(country)) %>%\n",
    "    select(age, sex, cp, trestbps, chol, restecg, thalach, country, num)\n",
    "\n",
    "num_counts <- data_tidy %>%\n",
    "    group_by(num) %>%\n",
    "    summarize(counts = n(), proportions = counts/nrow(data_tidy))\n",
    "\n",
    "num_counts\n",
    "head(data_tidy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ee319b-8f83-4336-96d2-cae8b12609fa",
   "metadata": {},
   "source": [
    "Evidently, the data is now much more balanced. Now that we have fully tidied our data (`data_tidy`), we can begin the variable selection portion of the analysis. First, we can make a covariance matrix between all continuous variables in order to get a preliminary look at whether or not there exist any inter-dependencies between variables (ie. colinearity). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4e52c2-1d0d-46ed-a13c-6c92708b9855",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width = 8, repr.plot.height = 8)\n",
    "\n",
    "covariance_plot <- data_tidy %>%\n",
    "    select(age, trestbps, chol, thalach, num) %>%\n",
    "    ggpairs(aes(colour = num, alpha = 0.5), lower = list(combo = wrap(\"facethist\", binwidth = 2)))\n",
    "\n",
    "covariance_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216319a1-6004-47c3-87bb-5ffa4998744a",
   "metadata": {},
   "source": [
    "The covariance plot above shows that the distribution for when `chol` = 1 is skewed. A skewed distribution can bias our predictions and model, thus further investigation into categorial variables, including `chol` has been performed in the form of a boxplot matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ab851d-f162-48d8-80f5-0d72e78cbad8",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width = 15, repr.plot.height = 10)\n",
    "\n",
    "numeric_variables <- colnames(select(data_tidy, where(is.numeric)))\n",
    "categorical_variables <- colnames(select(data_tidy, where(is.factor)))\n",
    "\n",
    "index <- 1\n",
    "\n",
    "for (j in categorical_variables) {\n",
    "    \n",
    "    count <- 1\n",
    "    for (i in numeric_variables) {\n",
    "        p <- data_tidy %>% ggplot() + \n",
    "            geom_boxplot(aes(x = .data[[j]], y = .data[[i]], fill = .data[[j]]))\n",
    "        \n",
    "        if (count == 1) {\n",
    "            p <- p + theme(legend.position = \"top\")\n",
    "        } else {\n",
    "            p <- p + theme(legend.position = \"none\")\n",
    "        } \n",
    "        if (index != 1) {\n",
    "            p <- p + \n",
    "                theme(axis.title.y=element_blank(),\n",
    "                axis.text.y=element_blank(),\n",
    "                axis.ticks.y=element_blank())\n",
    "        } \n",
    "        if (count != 4) {\n",
    "            p <- p + \n",
    "                theme(axis.title.x=element_blank(),\n",
    "                axis.text.x=element_blank(),\n",
    "                axis.ticks.x=element_blank())\n",
    "        } \n",
    "        \n",
    "        assign(paste0(\"plot\", toString(count)), p)\n",
    "        count <- count + 1\n",
    "    }\n",
    "    \n",
    "    g <- plot_grid(plot1, plot2, plot3, plot4, ncol = 1)\n",
    "    assign(paste0(\"g\", toString(index)), g)\n",
    "    index <- index + 1\n",
    "}\n",
    "\n",
    "plot_grid(g1, g2, g3, g4, g5, ncol = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756e729e-ee4a-424b-beea-702ce8110176",
   "metadata": {},
   "source": [
    "Firstly, we can see that in all plots with `chol`, there are many observations with `chol` = 0. Naturally, our tidy data will be prone to outliers as no form of data collection is fully robust, however, upon further inspection of the graphs above, we can see that there are so many values of `chol` equal to 0 that the quartiles of the boxplots get skewed. Cholesterol performs important roles, such as making hormones, thus our bodies require cholesterol<sup>3</sup>. This means that logically, it does not make sense for your body to have a 0 mg/dL of cholesterol. \n",
    "\n",
    "Therefore, since this measurement is likely a mistake in the data collection process, and is skewing the distribution (including with the mean and standard deviation) of the variable, we have no choice but to remove all rows with `chol` being 0. Showcasing the new summary table after removing the outliers, we get the following. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d84740-bebe-44b2-add0-a6bd760070cc",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "summary_table <- data_tidy %>%\n",
    "    mutate(across(age:num, ~as.numeric(as.character(.x)))) %>%\n",
    "    select(-num) %>%\n",
    "    pivot_longer(cols = age:country, \n",
    "                names_to = \"variable\", \n",
    "                values_to = \"value\") %>%\n",
    "    group_by(variable) %>%\n",
    "    summarize(mean = mean(value), sd = sd(value), max = max(value), min = min(value))\n",
    "\n",
    "t(summary_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba4e91b-0b07-41ec-a63d-9f3d4043ed56",
   "metadata": {},
   "source": [
    "## Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ab2764-96f4-4fc2-98a2-192bda86bfeb",
   "metadata": {},
   "source": [
    "**Reliability of Report**\n",
    "\n",
    "Our report is trustworthy firstly because it is easily reproducible and the data is open source. Many studies comparing the accuracy of predictive models in diagnosing heart disease have used the UCI Heart Disease data set, in particular the Cleveland one<sup>2,7,9</sup>, which suggests that this data set is quite valid and reliable. Moreover, the data set contains information from 3 countries, perhaps reflecting a better representation of our diverse community, and the data size is large, meaning we are more likely to produce better estimates. Lastly, we use reliable sources from the government and past peer-reviewed literature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc32f10-5664-4d48-ab67-a1d2b8d24823",
   "metadata": {},
   "source": [
    "**Plan**\n",
    "\n",
    "In this project, there are 2 things we expect to achieve: 1. A set of pairs of variables that are highly correlated to each other, and the analysis of their impact to the predictive power of a logistic model, and 2. A model that is able to predict whether a person has heart disease or not. Our results can provide insights into the relationships between different health indicators, and generate a model to make predictions about heart disease. To do so, we will carry out our project in two parts. \n",
    "\n",
    "In the first part, we will use a covariance/correlation plot of all explanatory variables to illustrate the relationship and dependencies between each pair of them. This process will end up with a set of pairs of explanatory variables with high correlations (standards will be discussed in detail in the project report). Hence, we can choose a set of explanatory variables (denoted as E)  for further regression and model selection.\n",
    "\n",
    "In the second part, we will use ridge logistic regression to fit the data, and perform a 10-fold cross-validation to estimate the test error of the models with different tuning parameters lambda, and select the lambda and sets of predictors corresponding to the model with least CV score  (i.e. least estimated test error rate). We will not use Cp, AIC and BIC because we do not have access to the population variance and do not know whether we have an unbiased estimate. By doing so, we are able to find the best (or approximately the best) model that predicts the occurrence of heart disease. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ced1ee-53ef-4f7f-98e4-37cbc0462a2a",
   "metadata": {},
   "source": [
    "**Implications**\n",
    "\n",
    "Being able to produce a model that accurately predicts and diagnoses heart disease can truly benefit patients. It will help identify individuals who have signs of heart disease and prevent further development of the disease, decreasing the number of deaths related to heart disease. Ultimately, an accurate model would help improve the quality of health services for patients. It will have preventative and predictive applications in health care."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a96a85e-2c5a-475d-a0b5-a0849eb0bea2",
   "metadata": {},
   "source": [
    "# References "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40844d53-48b0-4368-89f5-f4f4a4c62aea",
   "metadata": {},
   "source": [
    "<sup>1</sup> Abdar, M., Książek, W., Acharya, U. R., Tan, R.-S., Makarenkov, V., & Pławiak, P. (2019). A new machine learning technique for an accurate diagnosis of coronary artery disease. Computer Methods and Programs in Biomedicine, 179, 104992. https://doi.org/10.1016/j.cmpb.2019.104992\n",
    "\n",
    "<sup>2</sup> Akella, A., & Akella, S. (2021). Machine learning algorithms for predicting coronary artery disease: efforts toward an open source solution. Future Science OA, 7(6), FSO698. https://doi.org/10.2144/fsoa-2020-0206\n",
    "\n",
    "<sup>3</sup> CDC. (2021, January 26). Cholesterol Myths and Facts. Centers for Disease Control and Prevention. https://www.cdc.gov/cholesterol/myths_facts.htm\n",
    "\n",
    "‌\n",
    "<sup>4</sup> Centers for Disease Control and Prevention. (2020). Heart disease in the United States\n",
    "https://www.cdc.gov/heartdisease/facts.htm  \n",
    "\n",
    "<sup>5</sup> Mayo Clinic. (2021, February 9). Heart disease - Symptoms and causes. Mayo Clinic. https://www.mayoclinic.org/diseases-conditions/heart-disease/symptoms-causes/syc-20353118\n",
    "\n",
    "<sup>6</sup> Muhammad, L. J., Al-Shourbaji, I., Haruna, A. A., Mohammed, I. A., Ahmad, A., & Jibrin, M. B. (2021). Machine Learning Predictive Models for Coronary Artery Disease. SN Computer Science, 2(5). https://doi.org/10.1007/s42979-021-00731-4\n",
    "\n",
    "<sup>7</sup> Shah, D., Patel, S., & Bharti, S. K. (2020). Heart Disease Prediction using Machine Learning Techniques. International Journal of Innovative Technology and Exploring Engineering, 9(5), 1456–1460. https://doi.org/10.35940/ijitee.e2862.039520\n",
    "\n",
    "<sup>8</sup> UCI Machine Learning Repository: Heart Disease Data Set. (2019). Uci.edu. https://archive.ics.uci.edu/ml/datasets/Heart+Disease \n",
    "\n",
    "<sup>9</sup> Verma, L., Srivastava, S., & Negi, P. C. (2016). A Hybrid Data Mining Model to Predict Coronary Artery Disease Cases Using Non-Invasive Clinical Data. Journal of Medical Systems, 40(7). https://doi.org/10.1007/s10916-016-0536-z\n",
    "\n",
    "<sup>10</sup> Wang, C., Zhao, Y., Jin, B., Gan, X., Liang, B., Xiang, Y., Zhang, X., Lu, Z., & Zheng, F. (2021). Development and Validation of a Predictive Model for Coronary Artery Disease Using Machine Learning. Frontiers in Cardiovascular Medicine, 8. https://doi.org/10.3389/fcvm.2021.614204\n",
    "\n",
    "‌<sup>11</sup> World Health Organization. (2021, June 11). Cardiovascular Diseases (CVDs). Who.int; World Health Organization: WHO. https://www.who.int/news-room/fact-sheets/detail/cardiovascular-diseases-(cvds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076bfbb9-ec8a-4b62-8550-26a7e5fe2fea",
   "metadata": {},
   "source": [
    "# Appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cddb9f-bfd9-424f-9683-7d8f5282e375",
   "metadata": {},
   "source": [
    "## Appendix I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa42107-f886-401d-8de0-71d007dada38",
   "metadata": {},
   "source": [
    "\n",
    "![Table 1: Dataset variable names, types, and descriptions](Project/Variable_table.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
